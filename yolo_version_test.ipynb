{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo-version-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNy/B0L3UQpOEM/DNPkHHZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c34f80ce0a446afa3e3f90e7fa54802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92c6f21ad47741659993df5dc4f486cf",
              "IPY_MODEL_f7f5cf3f7b764949a8503731bcf63783"
            ],
            "layout": "IPY_MODEL_eb3fe5c2ff5c46289d014168929b97d7"
          }
        },
        "92c6f21ad47741659993df5dc4f486cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a0a64afbcd43a2bebf3312f00db3be",
            "max": 124240081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d56773cff7f4e3aafa8ad085d3bc001",
            "value": 124240081
          }
        },
        "f7f5cf3f7b764949a8503731bcf63783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b5a0ff8d694f248b09edf3e41a8daa",
            "placeholder": "​",
            "style": "IPY_MODEL_5dad9155496749c38c5797000f263cad",
            "value": " 118M/118M [00:02&lt;00:00, 62.1MB/s]"
          }
        },
        "eb3fe5c2ff5c46289d014168929b97d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7a0a64afbcd43a2bebf3312f00db3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d56773cff7f4e3aafa8ad085d3bc001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f0b5a0ff8d694f248b09edf3e41a8daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dad9155496749c38c5797000f263cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhlee508/Colab/blob/master/yolo_version_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mbCUVVlyq-G"
      },
      "source": [
        "import torch\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774,
          "referenced_widgets": [
            "7c34f80ce0a446afa3e3f90e7fa54802",
            "92c6f21ad47741659993df5dc4f486cf",
            "f7f5cf3f7b764949a8503731bcf63783",
            "eb3fe5c2ff5c46289d014168929b97d7",
            "a7a0a64afbcd43a2bebf3312f00db3be",
            "4d56773cff7f4e3aafa8ad085d3bc001",
            "f0b5a0ff8d694f248b09edf3e41a8daa",
            "5dad9155496749c38c5797000f263cad"
          ]
        },
        "id": "Ei_trY8Hyyfi",
        "outputId": "b9602d47-719e-4597-dd0f-6bc08cf61b04"
      },
      "source": [
        "modelv3 = torch.hub.load('ultralytics/yolov3', 'yolov3')  # or 'yolov3_spp', 'yolov3_tiny'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov3/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, [1, 1]]           \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61949149 parameters, 61949149 gradients\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.5.0/yolov3.pt to yolov3.pt...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c34f80ce0a446afa3e3f90e7fa54802",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=124240081.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "YOLOv3 🚀 2021-4-28 torch 1.8.1+cu101 CPU\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding autoShape... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BPzubFGz9JM",
        "outputId": "d854df10-d383-484b-c2b7-63bdc8242bc7"
      },
      "source": [
        "modelv5x = torch.hub.load('ultralytics/yolov5', 'yolov5x')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (5.4.1)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
            "Adding autoShape... \n",
            "YOLOv5 🚀 2021-5-12 torch 1.8.1+cu101 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCetV3X3zBdi",
        "outputId": "2cd49fe1-46c5-460e-d085-564ec222f8a9"
      },
      "source": [
        "test = modelv5x(\"/content/car2.jpg\")\n",
        "test.show()\n",
        "# test.save()\n",
        "print(\"class = \", test.names[int(test.xyxy[0][0][-1])])\n",
        "print(\"x =\", (int(test.xyxy[0][0][0]) + int(test.xyxy[0][0][2]))/2)\n",
        "print(\"y =\", (int(test.xyxy[0][0][1]) + int(test.xyxy[0][0][3]))/2)\n",
        "# print('\\n', test.xyxy[0][0])\n",
        "print(test.pandas().xyxy[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class =  car\n",
            "x = 255.0\n",
            "y = 140.0\n",
            "        xmin       ymin        xmax        ymax  confidence  class name\n",
            "0  15.400001  29.300001  495.200012  251.100006    0.919922      2  car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAOCR76ny0N6"
      },
      "source": [
        "# Git Clone YOLOv5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfH4rSHAjIG_",
        "outputId": "9e4f9c34-ba3f-4fa2-a765-bed80b0fb377"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 6260, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 6260 (delta 24), reused 38 (delta 18), pack-reused 6206\u001b[K\n",
            "Receiving objects: 100% (6260/6260), 8.52 MiB | 8.73 MiB/s, done.\n",
            "Resolving deltas: 100% (4279/4279), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp2nNwoNjK7w",
        "outputId": "20d90146-5dd8-4db5-85c7-af7e5aba0a6c"
      },
      "source": [
        "%cd /content/yolov5/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.9.1+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (2.4.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.1.5)\n",
            "Collecting thop\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/8b/22ce44e1c71558161a8bd54471123cc796589c7ebbfc15a7e8932e522f83/thop-0.0.31.post2005241907-py3-none-any.whl\n",
            "Requirement already satisfied: pycocotools>=2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (2.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.28.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0->-r requirements.txt (line 29)) (0.29.22)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.10.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (4.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.4.1->-r requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 15)) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 15)) (0.4.8)\n",
            "Installing collected packages: PyYAML, thop\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 thop-0.0.31.post2005241907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_AnwYIbjSfM",
        "outputId": "3acedcb3-e832-4a84-9fd9-b14f45fbe160"
      },
      "source": [
        "!python detect.py --source \"/content/traffic2 cut.mp4\" --weights yolov5x.pt --conf 0.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.6, device='', exist_ok=False, hide_conf=False, hide_labels=False, img_size=640, iou_thres=0.45, line_thickness=3, name='exp', nosave=False, project='runs/detect', save_conf=False, save_crop=False, save_txt=False, source='/content/traffic2 cut.mp4', update=False, view_img=False, weights=['yolov5x.pt'])\n",
            "YOLOv5 🚀 v5.0-81-gabfcf9e torch 1.8.1+cu101 CUDA:0 (Tesla K80, 11441.1875MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 476 layers, 87730285 parameters, 0 gradients\n",
            "video 1/1 (1/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.175s)\n",
            "video 1/1 (2/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.159s)\n",
            "video 1/1 (3/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.146s)\n",
            "video 1/1 (4/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.134s)\n",
            "video 1/1 (5/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (6/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (7/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (8/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (9/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (10/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (11/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (12/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (13/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (14/114) /content/traffic2 cut.mp4: 384x640 12 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (15/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (16/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (17/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (18/114) /content/traffic2 cut.mp4: 384x640 13 cars, Done. (0.131s)\n",
            "video 1/1 (19/114) /content/traffic2 cut.mp4: 384x640 12 cars, Done. (0.130s)\n",
            "video 1/1 (20/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (21/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (22/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, Done. (0.130s)\n",
            "video 1/1 (23/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, Done. (0.131s)\n",
            "video 1/1 (24/114) /content/traffic2 cut.mp4: 384x640 13 cars, Done. (0.131s)\n",
            "video 1/1 (25/114) /content/traffic2 cut.mp4: 384x640 12 cars, Done. (0.130s)\n",
            "video 1/1 (26/114) /content/traffic2 cut.mp4: 384x640 12 cars, Done. (0.130s)\n",
            "video 1/1 (27/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, Done. (0.131s)\n",
            "video 1/1 (28/114) /content/traffic2 cut.mp4: 384x640 12 cars, Done. (0.130s)\n",
            "video 1/1 (29/114) /content/traffic2 cut.mp4: 384x640 12 cars, Done. (0.130s)\n",
            "video 1/1 (30/114) /content/traffic2 cut.mp4: 384x640 1 person, 11 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (31/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (32/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (33/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (34/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, Done. (0.130s)\n",
            "video 1/1 (35/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (36/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (37/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (38/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (39/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 3 trucks, Done. (0.129s)\n",
            "video 1/1 (40/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (41/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (42/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (43/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (44/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (45/114) /content/traffic2 cut.mp4: 384x640 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (46/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 2 trucks, Done. (0.129s)\n",
            "video 1/1 (47/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 3 trucks, Done. (0.129s)\n",
            "video 1/1 (48/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (49/114) /content/traffic2 cut.mp4: 384x640 12 cars, 2 trucks, Done. (0.129s)\n",
            "video 1/1 (50/114) /content/traffic2 cut.mp4: 384x640 12 cars, 2 trucks, Done. (0.131s)\n",
            "video 1/1 (51/114) /content/traffic2 cut.mp4: 384x640 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (52/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (53/114) /content/traffic2 cut.mp4: 384x640 13 cars, 2 trucks, Done. (0.129s)\n",
            "video 1/1 (54/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 2 trucks, Done. (0.131s)\n",
            "video 1/1 (55/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (56/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (57/114) /content/traffic2 cut.mp4: 384x640 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (58/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (59/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (60/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (61/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (62/114) /content/traffic2 cut.mp4: 384x640 1 person, 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (63/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (64/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (65/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (66/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (67/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (68/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (69/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (70/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (71/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (72/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (73/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (74/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (75/114) /content/traffic2 cut.mp4: 384x640 2 persons, 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (76/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (77/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.129s)\n",
            "video 1/1 (78/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (79/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (80/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (81/114) /content/traffic2 cut.mp4: 384x640 12 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (82/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (83/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (84/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (85/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (86/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.132s)\n",
            "video 1/1 (87/114) /content/traffic2 cut.mp4: 384x640 1 person, 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (88/114) /content/traffic2 cut.mp4: 384x640 15 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (89/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (90/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (91/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (92/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (93/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (94/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.131s)\n",
            "video 1/1 (95/114) /content/traffic2 cut.mp4: 384x640 13 cars, 2 trucks, Done. (0.129s)\n",
            "video 1/1 (96/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.131s)\n",
            "video 1/1 (97/114) /content/traffic2 cut.mp4: 384x640 1 person, 15 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (98/114) /content/traffic2 cut.mp4: 384x640 1 person, 16 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (99/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.131s)\n",
            "video 1/1 (100/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.131s)\n",
            "video 1/1 (101/114) /content/traffic2 cut.mp4: 384x640 15 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (102/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (103/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (104/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (105/114) /content/traffic2 cut.mp4: 384x640 1 person, 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (106/114) /content/traffic2 cut.mp4: 384x640 14 cars, 2 trucks, Done. (0.130s)\n",
            "video 1/1 (107/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (108/114) /content/traffic2 cut.mp4: 384x640 14 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (109/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (110/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "video 1/1 (111/114) /content/traffic2 cut.mp4: 384x640 13 cars, 1 truck, Done. (0.130s)\n",
            "Results saved to runs/detect/exp18\n",
            "Done. (30.275s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR-pvuXaz8JT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}