{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cloth_Segm_u2net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhlee508/Colab/blob/master/Cloth_Segm_u2net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVVgm8h0itel",
        "outputId": "6775f03e-bd75-43a8-b7f7-747689bbd991"
      },
      "source": [
        "%cd /content/\n",
        "!rm -rf cloth-segmentation\n",
        "!git clone https://github.com/levindabhi/cloth-segmentation.git\n",
        "%cd cloth-segmentation\n",
        "!mkdir input_images\n",
        "!mkdir output_images\n",
        "!mkdir trained_checkpoint\n",
        "%cd trained_checkpoint/\n",
        "!gdown --id 1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'cloth-segmentation'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 62 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "/content/cloth-segmentation\n",
            "/content/cloth-segmentation/trained_checkpoint\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mhF3yqd7R-Uje092eypktNl-RoZNuiCJ\n",
            "To: /content/cloth-segmentation/trained_checkpoint/cloth_segm_u2net_latest.pth\n",
            "177MB [00:01, 145MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvP16OtiKHu"
      },
      "source": [
        "# Upload input images in *input_images* folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "2dkguFjplAkj",
        "outputId": "79ed98df-5428-42fb-deff-ce0ef0a2f642"
      },
      "source": [
        "from google.colab import files\n",
        "%cd ../input_images\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cloth-segmentation/input_images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-521eb7b6-74cf-4aad-aa70-8aef54207970\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-521eb7b6-74cf-4aad-aa70-8aef54207970\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving KakaoTalk_20210909_155907338.jpg to KakaoTalk_20210909_155907338.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHOOvOmodRLO",
        "outputId": "fd94d9e6-052a-4840-ce8a-cbe4bef04899"
      },
      "source": [
        "%cd /content/cloth-segmentation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cloth-segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6grKSDJcyNA"
      },
      "source": [
        "!python infer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8oAWI5cFxoe"
      },
      "source": [
        "# Infer Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA_ymqMFFzW9",
        "outputId": "f54c36d8-a982-4398-f8d8-bfc9d83a53dc"
      },
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from data.base_dataset import Normalize_image\n",
        "from utils.saving_utils import load_checkpoint_mgpu\n",
        "\n",
        "from networks import U2NET\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "image_dir = \"input_images\"\n",
        "result_dir = \"output_images\"\n",
        "checkpoint_path = os.path.join(\"trained_checkpoint\", \"cloth_segm_u2net_latest.pth\")\n",
        "do_palette = False\n",
        "\n",
        "\n",
        "def get_palette(num_cls):\n",
        "    \"\"\"Returns the color map for visualizing the segmentation mask.\n",
        "    Args:\n",
        "        num_cls: Number of classes\n",
        "    Returns:\n",
        "        The color map\n",
        "    \"\"\"\n",
        "    n = num_cls\n",
        "    palette = [0] * (n * 3)\n",
        "    for j in range(0, n):\n",
        "        lab = j\n",
        "        palette[j * 3 + 0] = 0\n",
        "        palette[j * 3 + 1] = 0\n",
        "        palette[j * 3 + 2] = 0\n",
        "        i = 0\n",
        "        while lab:\n",
        "            palette[j * 3 + 0] |= ((lab >> 0) & 1) << (7 - i)\n",
        "            palette[j * 3 + 1] |= ((lab >> 1) & 1) << (7 - i)\n",
        "            palette[j * 3 + 2] |= ((lab >> 2) & 1) << (7 - i)\n",
        "            i += 1\n",
        "            lab >>= 3\n",
        "    return palette\n",
        "\n",
        "\n",
        "transforms_list = []\n",
        "transforms_list += [transforms.ToTensor()]\n",
        "transforms_list += [Normalize_image(0.5, 0.5)]\n",
        "transform_rgb = transforms.Compose(transforms_list)\n",
        "\n",
        "net = U2NET(in_ch=3, out_ch=4)\n",
        "net = load_checkpoint_mgpu(net, checkpoint_path)\n",
        "net = net.to(device)\n",
        "net = net.eval()\n",
        "\n",
        "palette = get_palette(4)\n",
        "\n",
        "images_list = sorted(os.listdir(image_dir))\n",
        "pbar = tqdm(total=len(images_list))\n",
        "\n",
        "for image_name in images_list:\n",
        "    img = Image.open(os.path.join(image_dir, image_name)).convert(\"RGB\")\n",
        "    image_tensor = transform_rgb(img)\n",
        "    image_tensor = torch.unsqueeze(image_tensor, 0)\n",
        "\n",
        "    output_tensor = net(image_tensor.to(device))\n",
        "    output_tensor = F.log_softmax(output_tensor[0], dim=1)\n",
        "    output_tensor = torch.max(output_tensor, dim=1, keepdim=True)[1]\n",
        "    output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "    output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "    output_arr = output_tensor.cpu().numpy()\n",
        "    x = output_arr\n",
        "\n",
        "    # Change to RGB\n",
        "    output_arr = np.repeat(output_arr[:, :, np.newaxis], 3, axis=2)\n",
        "    output_arr[np.where((output_arr==[1, 1, 1]).all(axis=2))] = [0, 0, 255]\n",
        "    output_arr[np.where((output_arr==[2, 2, 2]).all(axis=2))] = [255, 0, 0]\n",
        "\n",
        "    output_img = Image.fromarray(output_arr.astype(\"uint8\"), mode=\"RGB\")\n",
        "\n",
        "    if do_palette:\n",
        "        output_img.putpalette(palette)\n",
        "    output_img.save(os.path.join(result_dir, image_name[:-3] + \"jpg\"))\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----checkpoints loaded from path: trained_checkpoint/cloth_segm_u2net_latest.pth----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3487: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oO49yvMc6dJ"
      },
      "source": [
        "!rm -r input_images/.ipynb_checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nN8p8pBuT-"
      },
      "source": [
        "# Output Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZAzyus2Bwm8"
      },
      "source": [
        "output_np = x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrc5HTvjCTWg",
        "outputId": "461e58fc-b63f-4330-bc45-990e8e0b5250"
      },
      "source": [
        "output_np.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXRp-_C4B5KF"
      },
      "source": [
        "# Output Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDhHWBILbAaj"
      },
      "source": [
        "%mkdir masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5DYAJj2B7Qc"
      },
      "source": [
        "# background\n",
        "back_np = output_np\n",
        "back_np = np.where(back_np != 0, 42, back_np)\n",
        "back_np = np.where(back_np == 0, 1, back_np)\n",
        "back_np = np.where(back_np == 42, 0, back_np)\n",
        "back_np = np.repeat(back_np[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "masked_background = Image.fromarray(back_np.astype(\"uint8\"), mode=\"RGB\")\n",
        "masked_background.save(\"masks/masked_background.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9dpaxcCgEq"
      },
      "source": [
        "# class 1\n",
        "class1_np = output_np\n",
        "class1_np = np.where(class1_np != 1, 42, class1_np)\n",
        "class1_np = np.where(class1_np == 1, 1, class1_np)\n",
        "class1_np = np.where(class1_np == 42, 0, class1_np)\n",
        "class1_np = np.repeat(class1_np[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "masked_class1 = Image.fromarray(class1_np.astype(\"uint8\"), mode=\"RGB\")\n",
        "masked_class1.save(\"masks/masked_class1.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq93kPA-RgSE"
      },
      "source": [
        "# class 2\n",
        "class2_np = output_np\n",
        "class2_np = np.where(class2_np != 2, 42, class2_np)\n",
        "class2_np = np.where(class2_np == 2, 1, class2_np)\n",
        "class2_np = np.where(class2_np == 42, 0, class2_np)\n",
        "class2_np = np.repeat(class2_np[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "masked_class2 = Image.fromarray(class2_np.astype(\"uint8\"), mode=\"RGB\")\n",
        "masked_class2.save(\"masks/masked_class2.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCtWoEhSRgs0"
      },
      "source": [
        "# class 3\n",
        "class3_np = output_np\n",
        "class3_np = np.where(class3_np != 3, 42, class3_np)\n",
        "class3_np = np.where(class3_np == 3, 1, class3_np)\n",
        "class3_np = np.where(class3_np == 42, 0, class3_np)\n",
        "class3_np = np.repeat(class3_np[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "masked_class3 = Image.fromarray(class1_np.astype(\"uint8\"), mode=\"RGB\")\n",
        "masked_class3.save(\"masks/masked_class3.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHiRwPtb4k1A"
      },
      "source": [
        "# Save Original Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G49woxRhPoYO"
      },
      "source": [
        "origin = img\n",
        "origin.save(\"origin.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1bQcQ8R4iaX"
      },
      "source": [
        "# Get Segmentation Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjzpuT1OceaQ"
      },
      "source": [
        "!mkdir imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Qe3nFeO5CS"
      },
      "source": [
        "# Background Seg Images\n",
        "back_imgs = back_np * img\n",
        "\n",
        "background_seg_imgs = Image.fromarray(back_imgs.astype(\"uint8\"), mode=\"RGB\")\n",
        "background_seg_imgs.save(\"imgs/background.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7sLh22kcnXJ"
      },
      "source": [
        "# Class1 Seg Images\n",
        "class1_imgs = class1_np * img\n",
        "\n",
        "class1_seg_imgs = Image.fromarray(class1_imgs.astype(\"uint8\"), mode=\"RGB\")\n",
        "class1_seg_imgs.save(\"imgs/class1.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7weu2Ak-coOu"
      },
      "source": [
        "# Class2 Seg Images\n",
        "class2_imgs = class2_np * img\n",
        "\n",
        "class2_seg_imgs = Image.fromarray(class2_imgs.astype(\"uint8\"), mode=\"RGB\")\n",
        "class2_seg_imgs.save(\"imgs/class2.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9DFSWXccolG"
      },
      "source": [
        "# Class3 Seg Images\n",
        "class3_imgs = class3_np * img\n",
        "\n",
        "class3_seg_imgs = Image.fromarray(class3_imgs.astype(\"uint8\"), mode=\"RGB\")\n",
        "class3_seg_imgs.save(\"imgs/class3.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6l2mRPzeb9e"
      },
      "source": [
        "### 단색에서 HSV 값  추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOzcr5Lid7Jw"
      },
      "source": [
        "%mkdir one_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49a7IhLJeEjH",
        "outputId": "1c33d432-2858-43ae-bb0c-c2d5ca1106b0"
      },
      "source": [
        "one_color = cv2.imread('check_color/purl.jpg') # 이미지 파일을 컬러로 불러옴\n",
        "height, width = one_color.shape[:2] # 이미지의 높이와 너비 불러옴, 가로 [0], 세로[1]\n",
        "\n",
        "one_hsv = cv2.cvtColor(one_color, cv2.COLOR_BGR2HSV) # cvtColor 함수를 이용하여 hsv 색공간으로 변환\n",
        "center_h = (height // 2)\n",
        "center_w = (width // 2)\n",
        "\n",
        "one_h = one_hsv[center_h][center_w][0]\n",
        "one_s = one_hsv[center_h][center_w][1]\n",
        "one_v = one_hsv[center_h][center_w][2]\n",
        "\n",
        "print(one_h, one_s, one_v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140 255 154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BkokaqyegEC"
      },
      "source": [
        "### 단색 입히기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO8KnuguGtcI"
      },
      "source": [
        "test = class1_imgs\n",
        "hsv = cv2.cvtColor(test.astype(\"uint8\"), cv2.COLOR_RGB2HSV)\n",
        "\n",
        "(h, s, v) = cv2.split(hsv)\n",
        "\n",
        "eval_s = s.sum() // np.count_nonzero(s)\n",
        "eval_v = v.sum() // np.count_nonzero(v)\n",
        "\n",
        "h[:, :] = one_h\n",
        "# s[:, :] = s + one_s - eval_s\n",
        "s[:, :] = np.where(s + (one_s - eval_s) > 255, 255, s + (one_s - eval_s))\n",
        "# v[:, :] = v + one_v - eval_v\n",
        "v[:, :] = np.where(v + (one_v - eval_v) > 255, 255, v + (one_v - eval_v))\n",
        "\n",
        "hsv = cv2.merge((h, s, v))\n",
        "\n",
        "rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "test = rgb * class1_np\n",
        "test = test + background_seg_imgs\n",
        "test = Image.fromarray(test.astype(\"uint8\"), mode=\"RGB\")\n",
        "test.save(\"imgs/one_color_changed.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-gHxzMs4MC"
      },
      "source": [
        "## 체크 무늬 입히기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUbSw4nhs7nc"
      },
      "source": [
        "!mkdir check_color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsDALJFjuKDc"
      },
      "source": [
        "check_color = cv2.imread('check_color/elastic_trans.jpg') \n",
        "check_rgb = cv2.cvtColor(check_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "check_np = class1_np * check_rgb\n",
        "check_person = check_np + background_seg_imgs\n",
        "\n",
        "check_seg_imgs = Image.fromarray(check_person.astype(\"uint8\"), mode=\"RGB\")\n",
        "check_seg_imgs.save(\"imgs/test.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdm2AA8C0jfG"
      },
      "source": [
        "check_color = cv2.imread('check_color/elastic_trans.jpg')\n",
        "height, width = check_color.shape[:2]\n",
        "\n",
        "check_hsv = cv2.cvtColor(check_color, cv2.COLOR_BGR2HSV)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvmBYC-t1XkK",
        "outputId": "6d6a2f8a-d153-4a32-8afb-9f441bdcc5de"
      },
      "source": [
        "check_hsv.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 500, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR6dYJRLzI0N"
      },
      "source": [
        "test = class1_imgs\n",
        "hsv = cv2.cvtColor(test.astype(\"uint8\"), cv2.COLOR_RGB2HSV)\n",
        "\n",
        "(h, s, v) = cv2.split(hsv)\n",
        "\n",
        "aver_s = s.sum() // np.count_nonzero(s)\n",
        "aver_v = v.sum() // np.count_nonzero(v)\n",
        "\n",
        "max_s = s.mean() \n",
        "max_v = v.mean() \n",
        "\n",
        "for i in range(h.shape[0]):\n",
        "    for j in range(h.shape[1]):\n",
        "        check_h = check_hsv[i][j][0] # h\n",
        "        check_s = check_hsv[i][j][1] # s\n",
        "        check_v = check_hsv[i][j][2] # v\n",
        "         \n",
        "        h[i][j] = check_h\n",
        "        s[i][j] = np.where(abs(s[i][j] + (check_s - aver_s)) > 255, 255, s[i][j] + abs((check_s - aver_s)))\n",
        "        #s[i][j] = s[i][j] + check_s\n",
        "        v[i][j] = np.where(abs(v[i][j] + (check_v - aver_v)) > 255, 255, abs(v[i][j] + (check_v - aver_v)))\n",
        "        #v[i][j] = v[i][j] + check_v\n",
        "\n",
        "hsv = cv2.merge((h, s, v))\n",
        "\n",
        "rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "test = rgb * class1_np\n",
        "test = test + background_seg_imgs\n",
        "test = Image.fromarray(test.astype(\"uint8\"), mode=\"RGB\")\n",
        "test.save(\"imgs/test2.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rA1TEoA5STV"
      },
      "source": [
        "## Elastic Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skSQZENN8TIa"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "def elastic_transform(image, alpha, sigma, random_state=None):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
        "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
        "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
        "       Proc. of the International Conference on Document Analysis and\n",
        "       Recognition, 2003.\n",
        "    \"\"\"\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState(None)\n",
        "\n",
        "    shape = image.shape\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dz = np.zeros_like(dx)\n",
        "\n",
        "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
        "    print(x.shape)\n",
        "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
        "\n",
        "    distorted_image = map_coordinates(image, indices, order=1, mode='reflect')\n",
        "    return distorted_image.reshape(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7stYsyIp68iH",
        "outputId": "d9350bde-9062-4376-cfbe-92f5358f3532"
      },
      "source": [
        "image = cv2.imread('check_color/check_clothe.jpg')\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "transformed = elastic_transform(image, alpha=700, sigma=9)\n",
        "transformed_image = Image.fromarray(transformed.astype(\"uint8\"), mode=\"RGB\")\n",
        "transformed_image.save(\"check_color/elastic_trans.jpg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciE2l5NJ5p_f",
        "outputId": "ca7124af-c2bf-440d-e447-ae463f42f04c"
      },
      "source": [
        "check_color.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 500, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTaqAnpT5Pkw"
      },
      "source": [
        "# Make Style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DEZJP2zK7Am"
      },
      "source": [
        "x = np.where(x != 1, 0, x)\n",
        "x = np.where(x != 0, 255, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C_wYhKw0XFO",
        "outputId": "26e04323-1c54-48be-eaf9-7e7c05041464"
      },
      "source": [
        "np.unique(x[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 255])"
            ]
          },
          "metadata": {},
          "execution_count": 518
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBBuW2YIJiC6"
      },
      "source": [
        "x = np.repeat(x[:, :, np.newaxis], 3, axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXjAU3bbHfVM",
        "outputId": "3296ecc6-d1ee-4fde-b7cb-2b08772b7643"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(456, 456, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 520
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEHXnknIJZ65"
      },
      "source": [
        "x = Image.fromarray(x.astype(\"uint8\"), mode=\"RGB\")\n",
        "x.save(\"person.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaobzeeE6SbN"
      },
      "source": [
        "# Upload Clothe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIO7XlSPGzWR"
      },
      "source": [
        "clothe = cv2.imread('clothe3.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKORYBUJG6ef"
      },
      "source": [
        "clothe = np.array(clothe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cLHypX1HABY",
        "outputId": "0dbe40df-8130-4aee-9ff5-d3f3fc60822d"
      },
      "source": [
        "clothe.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(456, 456, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3alSMgEN5Lpp"
      },
      "source": [
        "# Add Style & Background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN1NmI2tcPmz"
      },
      "source": [
        "added_style = clothe + seg\n",
        "\n",
        "back_image = Image.fromarray(seg_np.astype(\"uint8\"), mode=\"RGB\")\n",
        "back_image.save(\"seg_background.jpg\")\n",
        "\n",
        "added_style = Image.fromarray(added_style.astype(\"uint8\"), mode=\"RGB\")\n",
        "added_style.save(\"added_style.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz0H8x3VIPQ8"
      },
      "source": [
        "added_image = clothe * x\n",
        "\n",
        "added_style = added_image + seg_np\n",
        "\n",
        "added_image = Image.fromarray(added_image.astype(\"uint8\"), mode=\"RGB\")\n",
        "added_image.save(\"added_image.jpg\")\n",
        "\n",
        "added_style = Image.fromarray(added_style.astype(\"uint8\"), mode=\"RGB\")\n",
        "added_style.save(\"added_style.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqXHefoh5JuV"
      },
      "source": [
        "# Blended Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqzOX1bZNNol"
      },
      "source": [
        "blended = Image.blend(img, added_image, alpha=0.5)    \n",
        "blended.save(os.path.join(\"blended.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gsIJ44XcgYc"
      },
      "source": [
        "# Infer WebCam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkeHvvOHcgLO",
        "outputId": "47afbfe1-ac64-4303-bceb-2f64d5d558dc"
      },
      "source": [
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "import cv2\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from data.base_dataset import Normalize_image\n",
        "from utils.saving_utils import load_checkpoint_mgpu\n",
        "\n",
        "from networks import U2NET\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "checkpoint_path = os.path.join(\"trained_checkpoint\", \"cloth_segm_u2net_latest.pth\")\n",
        "do_palette = True\n",
        "\n",
        "\n",
        "def get_palette(num_cls):\n",
        "    \"\"\"Returns the color map for visualizing the segmentation mask.\n",
        "    Args:\n",
        "        num_cls: Number of classes\n",
        "    Returns:\n",
        "        The color map\n",
        "    \"\"\"\n",
        "    n = num_cls\n",
        "    palette = [0] * (n * 3)\n",
        "    for j in range(0, n):\n",
        "        lab = j\n",
        "        palette[j * 3 + 0] = 0\n",
        "        palette[j * 3 + 1] = 0\n",
        "        palette[j * 3 + 2] = 0\n",
        "        i = 0\n",
        "        while lab:\n",
        "            palette[j * 3 + 0] |= ((lab >> 0) & 1) << (7 - i)\n",
        "            palette[j * 3 + 1] |= ((lab >> 1) & 1) << (7 - i)\n",
        "            palette[j * 3 + 2] |= ((lab >> 2) & 1) << (7 - i)\n",
        "            i += 1\n",
        "            lab >>= 3\n",
        "    return palette\n",
        "\n",
        "\n",
        "transforms_list = []\n",
        "transforms_list += [transforms.ToTensor()]\n",
        "transforms_list += [Normalize_image(0.5, 0.5)]\n",
        "transform_rgb = transforms.Compose(transforms_list)\n",
        "\n",
        "net = U2NET(in_ch=3, out_ch=4)\n",
        "net = load_checkpoint_mgpu(net, checkpoint_path)\n",
        "net = net.to(device)\n",
        "net = net.eval()\n",
        "\n",
        "palette = get_palette(4)\n",
        "\n",
        "VideoSignal = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = VideoSignal.read()\n",
        "    if frame == None:\n",
        "        print(\"fail\")\n",
        "        break \n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    image_tensor = transform_rgb(img)\n",
        "    image_tensor = torch.unsqueeze(image_tensor, 0)\n",
        "\n",
        "    output_tensor = net(image_tensor.to(device))\n",
        "    output_tensor = F.log_softmax(output_tensor[0], dim=1)\n",
        "    output_tensor = torch.max(output_tensor, dim=1, keepdim=True)[1]\n",
        "    output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "    output_tensor = torch.squeeze(output_tensor, dim=0)\n",
        "    output_arr = output_tensor.cpu().numpy()\n",
        "    output_arr = np.array(output_arr, dtype=np.uint8)\n",
        "    result = frame * np.repeat(output_arr[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "    cv2.imshow('frame', result)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# When everything done, release the capture\n",
        "VideoSignal.release()\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----checkpoints loaded from path: trained_checkpoint/cloth_segm_u2net_latest.pth----\n",
            "fail\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtjAtdvig6u"
      },
      "source": [
        "# Download results from *output_images*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-W3khgQiz1K"
      },
      "source": [
        "!rm -rf output_images/*"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}